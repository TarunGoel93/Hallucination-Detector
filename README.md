# Hallucination Detector
[![Ask DeepWiki](https://devin.ai/assets/askdeepwiki.png)](https://deepwiki.com/TarunGoel93/Hallucination-Detector)

This repository contains a machine learning project to detect hallucinations in text generated by language models. The system compares a model's output (hypothesis) against a ground-truth source and/or target text to determine if the hypothesis contains fabricated information.

The project includes trained models, a Flask web application for interactive demonstration, and the original Jupyter notebook detailing the data processing, training, and evaluation pipeline. The final model, a Logistic Regression classifier using character-level n-grams, achieves *99.7% accuracy* on the test set.

## Features

- *High-Accuracy Model*: A scikit-learn Logistic Regression model trained on character n-grams to effectively capture textual discrepancies.
- *Interactive Web UI*: A Flask application (app.py) coupled with a feature-rich HTML frontend (index.html) for real-time analysis.
- *Textual Highlighting*: The web interface visually highlights which parts of the hypothesis match the source text and which are potential hallucinations.
- *Confidence Score*: Provides a probability score for each prediction, visualized with a dynamic speedometer gauge.
- *Multiple Approaches Explored*: The training notebook (HackLLMCode.ipynb) details several approaches, including TF-IDF with word n-grams and an LSTM-based deep learning model.

## Project Structure


└── hallucination-detector/
    ├── app.py                            # Flask web server to run the demo
    ├── index.html                        # Standalone HTML/CSS/JS frontend for the demo
    ├── HackLLMCode.ipynb                 # Jupyter Notebook with all data processing, training, and evaluation
    └── hallucination_detector_char_ngram.pkl  # The final trained Logistic Regression model


## Models & Training

The project was developed by experimenting with multiple classification methods.

### 1. Logistic Regression with TF-IDF (Final Model)

This model proved to be the most effective and efficient.
- *Vectorization*: TfidfVectorizer using character-level n-grams (from 2 to 5 characters). This approach is highly effective at capturing subtle differences, misspellings, and fabrications.
- *Classifier*: LogisticRegression with balanced class weights to handle data imbalance.
- *Performance: Achieved **99.7% accuracy* and a *0.9978 F1 score* on the test set.
- *File*: hallucination_detector_char_ngram.pkl

### 2. Recurrent Neural Network (LSTM)

An alternative approach was explored using deep learning.
- *Embeddings*: Text was converted into vector representations using the paraphrase-multilingual-MiniLM-L12-v2 Sentence Transformer model.
- *Architecture*: A simple LSTM classifier built with PyTorch.
- *Performance: Achieved a test accuracy of **96.7%*.

The complete exploration, including data cleaning, feature engineering, training loops, and evaluation metrics for all models, can be found in HackLLMCode.ipynb.

## How to Run the Demo

You can run the interactive hallucination detector in two ways.

### Option 1: Run the Flask Web Application

This method uses the pre-trained hallucination_detector_char_ngram.pkl model for predictions.

1.  *Clone the repository:*
    bash
    git clone https://github.com/TarunGoel93/Hallucination-Detector.git
    cd Hallucination-Detector
    

2.  *Install the required libraries:*
    bash
    pip install flask joblib scikit-learn pandas
    

3.  *Run the Flask application:*
    bash
    python app.py
    

4.  Open your web browser and navigate to http://localhost:5000.

### Option 2: Use the Standalone HTML File

The index.html file is a self-contained demo that uses client-side JavaScript to perform a heuristic-based analysis for highlighting.

1.  Clone the repository or download the index.html file.
2.  Open index.html directly in your web browser.
3.  Enter the source and hypothesis texts into the provided text areas and click "Analyze".

## How to Use the Detector

1.  *Source Text*: Paste the ground-truth or reference text into the first text area.
2.  *Hypothesis Text*: Paste the language model's output that you want to check into the second text area.
3.  *Analyze*: Click the "Analyze" button.

The interface will display:
- *Prediction*: A label indicating whether the text is a hallucination.
- *Confidence Score*: A probability score indicating the model's confidence.
- *Highlighted Text*: The hypothesis text will be color-coded. Matching parts are highlighted in green, and potential hallucinations are in red.
- *Token Counts*: The number of matched and hallucinated tokens.
